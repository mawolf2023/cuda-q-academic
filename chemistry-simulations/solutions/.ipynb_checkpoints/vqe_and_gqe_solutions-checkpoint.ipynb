{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de79f16-d273-4e0c-b1bd-24431816701d",
   "metadata": {},
   "source": [
    "# Gound State Preparation of Molecules Using VQE and GQE \n",
    "\n",
    "This notebook will provide an introduction to the ground state preparation problem on quantum computers and explore how approaches like the variational quantum eigensolver (VQE) are a good heuristic, but quickly run into problems. The rest of the lesson will introduce the Generative Quantum Eigensolver (GQE) through CUDA-Q Solver's easy to use API.  GQE is a technique that leverages AI to prepare ground states on the quantum computer with many benefits.\n",
    "\n",
    "**Prerequisites:** This notebook assumes a basic knowledge of quantum algorithms and topics like states, gates, circuits, etc.  If you are unfamiliar with these terms, you may want to complete our \"[Quick Start to Quantum](https://github.com/NVIDIA/cuda-q-academic/tree/main/quick-start-to-quantum)\" series. This notebook also assumes users have a basic understanding of some quantum chemical terminology such as molecular orbitals, molecular Hamiltonian, etc. \n",
    "\n",
    "**What you will do:**\n",
    "\n",
    "* Understand the importance of ground state prepartation.\n",
    "* Use VQE to prepare the ground state of a molecule.\n",
    "* Study the shortcomings of VQE.\n",
    "* Learn how the generative quantum eigensolver works.\n",
    "* Use GQE in CUDA-Q Solvers to prepare molecular ground states.\n",
    "* Explore different settings for GQE model training.\n",
    "\n",
    "**CUDA-Q Syntax you will use:**\n",
    "* cudaq.observe()\n",
    "* solvers.create_molecule\n",
    "* solvers.gqe()\n",
    "* solvers.get_pauli_pool()\n",
    "\n",
    "Run the cells below to load all the necessary packages for this notebook.  You may need to restart your kernel after doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed807bd2-b6ea-468e-ac61-c29144386ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are working in an environment that does not have cudaqx installed, \n",
    "# uncomment the code below to install cudaq-solvers and the required dependencies.  \n",
    "# Then restart the kernel before executing the next cell.\n",
    "#!sudo apt-get update && sudo apt-get install -y gfortran\n",
    "#!pip install cudaq-solvers -q\n",
    "\n",
    "!pip install cudaq-solvers -q\n",
    "import cudaq, cudaq_solvers as solvers\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "!pip install cudaq-solvers[gqe] -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a179ea-a58f-463a-ade9-b42b5ee790c8",
   "metadata": {},
   "source": [
    "## The Ground State Problem\n",
    "\n",
    "A fundamental quantity in quantum chemistry is the expectation value determined from a Hamiltonian operator $H$ and a quantum state (wavefunction) $\\ket{\\Psi}$.  The expectation value is defined as the average energy of the state $\\ket{\\Psi}$ and is represented with the following expression.\n",
    "\n",
    "$$E =  \\bra{\\Psi}H\\ket{\\Psi}$$ \n",
    "\n",
    "This energy can be computed for any arbitrary state, but is often most important when obtained from a particular state called the **ground state**. The ground state is defined as the state which produces the lowest expectation value. \n",
    "\n",
    "For chemists, the ground state energy of a molecule provides a wealth of knowledge about its reactivity and chemical properties as most molecules are in the ground state rather than excited states. For quantum computing more broadly, non-physics optimization problems can often be mapped to a Hamiltonian. The ground state of such a Hamiltonian, if obtained, can be sampled to produce high-quality solutions to the original optimization problem.  \n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/vqe_and_gqe/pes.png\" alt=\"plot\" width=\"600\">\n",
    "  <figcaption>Accurate ground state energies are necessary to compute the strength of a chemical bond and other important properties. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Identifying and preparing the gound state is an incredible challenge and is necessary to obtain quantities like the ground state energy. Consider an arbitrary quantum state constructed from $N$ qubits which is defined by a $2^N$ state vector of complex entries called amplitudes.  These amplitudes can take any complex value as long as they remain normalized.  \n",
    "\n",
    "$$\n",
    "|\\psi\\rangle =\n",
    "\\begin{pmatrix}\n",
    "\\alpha_0 \\\\\n",
    "\\alpha_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_{2^n-1}\n",
    "\\end{pmatrix}\n",
    "\\qquad\n",
    "\\text{with} \\qquad\n",
    "\\sum_{k=0}^{2^n-1} |\\alpha_k|^2 = 1\n",
    "$$\n",
    "\n",
    "The first issue arises from the exponential size of the quantum state.  Even if one knows all parameters of a given ground state exactly, it is impossible to store a complete state vector for systems of just over 50 qubits, even if we could pool the memory of every supercomputer in the world! \n",
    "\n",
    "Quantum computing subverts this issue using superposition and instead representing quantum states as circuits where a sub-exponential number of gates can be applied to $N$ qubits. See the example below. A 16 element state vector is constructed using only 8 gates applied to 4 qubits. \n",
    "\n",
    "<figure>\n",
    "  <img src=\"Images/vqe_and_gqe/sv.png\" alt=\"plot\" width=\"600\">\n",
    "</figure>\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 1:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "        Calculate the number of elements in the state vector and the number of gates required to prepare the same uniform superposition state for a system with 10 qubits and a system with 20 qubits.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "The second issue pertains to actually preparing a ground state with a quantum circuit. There is an infinite number of possible gate combinations and, without any intuition, blindly preparing a ground state circuit might be just as challenging as the original problem.  Cases like chemistry make this a bit easier because the underlying physics provides some structure , but finding the exact ground state may still require a prohibitively large number of gate operations for practical purposes.\n",
    "\n",
    "This is where approximate methods are important for preparing states that are \"close enough\" to the ground state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9f028-cf4f-4f1e-b698-ad3d6141022b",
   "metadata": {},
   "source": [
    "## Variational Methods and their Problems\n",
    "\n",
    "Thanks to the **variational theorem**, the search for a circuit that approximates the ground state can become a bit more systematic. The variational theorem states that for any normalized trial wavefunction $\\ket{\\Psi_{trial}}$ that\n",
    "\n",
    "$$\\bra{\\Psi_{trial}}H\\ket{\\Psi_{trial}} \\ge E_0, $$\n",
    "\n",
    "where $E_0$ is the ground state energy. This means that any trial wavefunction will produce an energy that is an upper bound to the ground state energy. In other words, we can try any trial wavefunction and the lowest energy result is the best approximation of the ground state.  This at least provides a way to rank our trial wavefunctions.\n",
    "\n",
    "This fact has inspired an entire class of quantum algorithms called **variational algorithms** which take advantage of the strengths of both quantum and classical processors working in tandem. The image below depicts the workflow of a general variational algorithm. \n",
    "\n",
    "<img src=\"Images/vqe_and_gqe/variational-diagram.png\" alt=\"plot\" width=\"900\">\n",
    "\n",
    "Th steps are as follows: \n",
    "\n",
    "1. A predetermined structure or **(ansatz)** of a circuit is chosen where some of the rotation gates are parameterized ($\\Theta={\\theta_0 \\cdots \\theta_n}$).\n",
    "2. The parameters are then randomly initialized (i.e., $\\Theta$ is set to $\\Theta_0$).\n",
    "3. An expectation value is computed on a quantum computer using the $\\bra{\\Psi(\\Theta)}H\\ket{\\Psi(\\Theta)}$.\n",
    "4. The resulting energy is fed into a classical optimizer which updates the parameter values $\\Theta$.\n",
    "5. Steps 3 and 4 are repeated until convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b92da1-631a-48e0-9e70-6bc63171452c",
   "metadata": {},
   "source": [
    "Despite the ingenuity behind variational algorithms, there are many problems that limit their effectiveness.  One of the most documented is the so-called barren plateau problem.  For even small cases, like the 14-qubit example taken from [\"Quantum variational algorithms are swamped with traps\"](https://www.nature.com/articles/s41467-022-35364-5) shown below, the loss function landscape is extremely challenging, and, at problem sizes of even moderate size, the landscape usually becomes too flat to optimize. \n",
    "\n",
    " <img src=\"Images/vqe_and_gqe/surface.png\" alt=\"plot\" width=\"900\">\n",
    "\n",
    "We have also assumed evaluation of $\\bra{\\Psi_{trial}}H\\ket{\\Psi_{trial}}$ is efficient on a QPU in the sense that it is possible to obtain expectation values for states much larger than ever possible with exact classical methods.  However, the wallclock time to run these on a QPU may still be quite costly and produce a practical limitation for VQE.  The barren plateau problem makes this problem worse as larger molecule require many more steps to converge the energy, if it will converge at all.  Gradients can help, but are extremely costly to evaluate in their own right.\n",
    "\n",
    "Consider the image below.  $H_2$ quickly converges as it has only three parameters, but even a small molecule like $N_2$ has 609 parameters and is far from converged after 300 iterations.  Considering the exact energy of these molecules can easily be computed exactly with matrix diagonalization and convergence issues are already becoming apparent means systems that require large number of qubits quickly become prohibitive to solve.\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 2:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "        Use the CUDA-Q Solvers code below to run VQE for $H_2$, $LiH$, and $N_2$ with a standard unitary coupled cluster with singled and double excitations (UCCSD) ansatz.  Note how many parameters are required for each system and comment on the convergence behavior.   Note, that these are still very small molecules with small basis sets.  Try playing around with the function `stateprep.get_num_uccsd_parameters` with different qubit counts and number of electrons to see how parameter numbers can quickly get out of hand.\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0f1dd5-c2fd-40d0-9f43-695bb39822ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "error encountered when launching pyscf molecule generation server.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m geometry = [(\u001b[33m'\u001b[39m\u001b[33mH\u001b[39m\u001b[33m'\u001b[39m, (\u001b[32m0.\u001b[39m, \u001b[32m0.\u001b[39m, \u001b[32m0.\u001b[39m)), (\u001b[33m'\u001b[39m\u001b[33mH\u001b[39m\u001b[33m'\u001b[39m, (\u001b[32m0.\u001b[39m, \u001b[32m0.\u001b[39m, \u001b[32m0.74\u001b[39m))]\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#geometry = [('Li', (0., 0., 0.)), ('H', (0., 0., 1.59))]\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#geometry = [('N', (0., 0., 0.)), ('N', (0., 0., 1.10))]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m molecule = \u001b[43msolvers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_molecule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msto-3g\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCASCI\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get the number of qubits and electrons\u001b[39;00m\n\u001b[32m      9\u001b[39m numQubits = molecule.n_orbitals * \u001b[32m2\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: error encountered when launching pyscf molecule generation server."
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the molecular hamiltonian\n",
    "geometry = [('H', (0., 0., 0.)), ('H', (0., 0., 0.74))]\n",
    "#geometry = [('Li', (0., 0., 0.)), ('H', (0., 0., 1.59))]\n",
    "#geometry = [('N', (0., 0., 0.)), ('N', (0., 0., 1.10))]\n",
    "molecule = solvers.create_molecule(geometry, 'sto-3g', 0, 0, CASCI = True)\n",
    "\n",
    "\n",
    "# Get the number of qubits and electrons\n",
    "numQubits = molecule.n_orbitals * 2\n",
    "numElectrons = molecule.n_electrons\n",
    "\n",
    "spin = 0\n",
    "initialX = [-.2] * solvers.stateprep.get_num_uccsd_parameters(\n",
    "    numElectrons, numQubits)\n",
    "\n",
    "\n",
    "# Define the UCCSD ansatz\n",
    "@cudaq.kernel\n",
    "def ansatz(thetas: list[float]):\n",
    "    q = cudaq.qvector(numQubits)\n",
    "    for i in range(numElectrons):\n",
    "        x(q[i])\n",
    "    solvers.stateprep.uccsd(q, thetas, numElectrons, spin)\n",
    "\n",
    "\n",
    "def cost(theta):\n",
    "\n",
    "    exp_val = cudaq.observe(ansatz, molecule.hamiltonian, theta).expectation()\n",
    "\n",
    "    return exp_val\n",
    "\n",
    "exp_vals = []\n",
    "\n",
    "\n",
    "def callback(xk):\n",
    "    exp_vals.append(cost(xk))\n",
    "\n",
    "\n",
    "cudaq.set_target(\"nvidia\", option = 'fp64')\n",
    "result = minimize(cost,\n",
    "                  initialX,\n",
    "                  method='COBYLA',\n",
    "                  callback=callback,\n",
    "                  options={'maxiter': 300})\n",
    "\n",
    "print('UCCSD-VQE energy =  ', result.fun)\n",
    "print('Number of Variational Parameters =', len(initialX))\n",
    "\n",
    "plt.plot(exp_vals)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('VQE')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(cudaq.sample(ansatz, result.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e02bbb-c074-4550-9ff5-ae3194024199",
   "metadata": {},
   "source": [
    "Another issue is ansatz selection. The quality of the result is limited by how expressive the selected ansatz is. In the case of $H_2$, the ground state can be obtained exactly, because the UCCSD ansatz provides sufficient entanglement structure to match the exact full configuration interaction (FCI) result. \n",
    "\n",
    "This is not necessarily true if we select some other ansatz.  \n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 3:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "        Use the code below to specify a custom ansatz in the kernel below in the following manner. Apply $H$ gates to the two occupied qubits.  Add CNOT gates between each adjacent qubit pair and then apply a parameterized $R_X$, $R_Y$, and $R_Z$ to each qubit.  This should result in a VQE with 12 parameters, 6 times more than UCCSD.  Comment on the convergence of the problem.  How does the final energy compare to the FCI energy? Why might this be the result? Hint: Try running `cudaq.sample()` for the converged UCCSD result and for your ansatz.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb857e-11cf-48a0-987c-40f5629dea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudaq.set_target(\"nvidia\", option = 'fp64')\n",
    "geometry = [('H', (0., 0., 0.)), ('H', (0., 0., 0.74))]\n",
    "\n",
    "molecule = solvers.create_molecule(geometry, 'sto-3g', 0, 0, casci = True)\n",
    "\n",
    "print(molecule.energies)\n",
    "\n",
    "# Get the number of qubits and electrons\n",
    "numQubits = molecule.n_orbitals * 2\n",
    "numElectrons = molecule.n_electrons\n",
    "\n",
    "spin = 0\n",
    "initialX = [-.3] * 12\n",
    "\n",
    "@cudaq.kernel\n",
    "def custom_ansatz(thetas: list[float]):\n",
    "    q = cudaq.qvector(numQubits)\n",
    "\n",
    "    x(q[0]) #prepares Hartree Fock State\n",
    "    x(q[1])\n",
    "\n",
    "    h(q[0]) #prepares Hartree Fock State\n",
    "    h(q[1])\n",
    "\n",
    "    #TODO START\n",
    "    for i in range(numQubits):\n",
    "        x.ctrl(q[i], q[(i+1)%numQubits])\n",
    "\n",
    "    for i in range(numQubits):\n",
    "        ry(thetas[i], q[i])\n",
    "\n",
    "    for i in range(numQubits):\n",
    "        rx( thetas[i+4], q[i])\n",
    "\n",
    "    for i in range(numQubits):\n",
    "        rz(thetas[i+8], q[i])\n",
    "\n",
    "\n",
    "\n",
    "print(cudaq.draw(custom_ansatz, initialX))\n",
    "        \n",
    "\n",
    "def cost(theta):\n",
    "\n",
    "    exp_val = cudaq.observe(custom_ansatz, molecule.hamiltonian, theta).expectation()\n",
    "\n",
    "    return exp_val\n",
    "\n",
    "exp_vals = []\n",
    "\n",
    "\n",
    "def callback(xk):\n",
    "    exp_vals.append(cost(xk))\n",
    "\n",
    "cudaq.set_target('nvidia')\n",
    "result = minimize(cost,\n",
    "                  initialX,\n",
    "                  method='COBYLA',\n",
    "                  callback=callback,\n",
    "                  options={'maxiter': 300})\n",
    "\n",
    "print('Custom Ansatz Energy =  ', result.fun)\n",
    "print('Number of Variational Parameters =', len(initialX))\n",
    "print('Variational Parameters =', result.x)\n",
    "\n",
    "print(cudaq.draw(custom_ansatz, result.x))\n",
    "\n",
    "plt.plot(exp_vals)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Energy')\n",
    "plt.title('VQE')\n",
    "plt.show()\n",
    "\n",
    "#TODO\n",
    "print(cudaq.sample(custom_ansatz, result.x)) #does not preserve particle number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6807be-1fd9-43d2-b661-155e06102dbc",
   "metadata": {},
   "source": [
    "Chemistry provides some good intuition for ansatz selection, but this starts to break down with larger, strongly correlated molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97569440-5d4b-4abf-b2c2-4628d62e5e76",
   "metadata": {},
   "source": [
    "## The Generative Quantum Eigensolver\n",
    "\n",
    "In an attempt to address the issues posed by VQE for finding molecular ground states, researchers from NVIDIA, U. Toronto, and St Jude Children's Research Hospital developed a technique called the generative quantum eigensolver (GQE) which uses a generative pretrained transformer (GPT) model to sample circuits which approximate a molecule's ground state. \n",
    "\n",
    "This section will explain some of the technical details of GQE, but first consider how this compares to VQE at a high level. The diagram below shows the two workflows. \n",
    "\n",
    " <img src=\"Images/vqe_and_gqe/gqe_vqe.png\" alt=\"plot\" width=\"900\">\n",
    "Notice that both are similar in the sense that a quantum circuit is prepared and sampled to produce an energy which informs how the model parameters are updated.  The key difference is that with GQE, the optimized parameters are the neural network parameters of the GPT model and not within the quantum circuits.  This provides a much better optimization landscape, avoiding the barren plateau issue. There are further benefits related to pretraining, but these will be discussed later.\n",
    "\n",
    "Let's zoom in a bit on how the GPT model works and circuits are sampled.  First, the model needs a vocabulary.  The natural choice is a set of time evolution operators derived from a chemically inspired ansatz like UCCSD.  In the paper, the authors have a large set of operators of the form $e^{iP_jt_k}$ where $P_j$ is an operator from UCCSD and  $t_k$ is some a time step selected from a range of possible time steps. \n",
    "\n",
    "Just as ansatz selection for VQE was critical for convergence, a good vocabulary is key for GQE.  GQE just needs to be given an vocabulary that is flexible enough. It is also possible to select a vocabulary tuned to specific qubit modalities such as those with nearest neighbor connectivity.\n",
    "\n",
    "\n",
    "The sampling process is shown in the figure below.  The GPT model takes the parameters, the vocabulary, an a partially constructed circuits (initially an empty circuit) as inputs.  The GPT model then outputs a logit vector related to the probability of sampling index $i$ corresponding to the $i$-th unitary in the vocabulary.  This index is then appended to the string and fed back into the GPT model until a sequence of $N$ (user specified) indices is created, i.e. the full circuit. \n",
    "\n",
    "\n",
    " <img src=\"Images/vqe_and_gqe/gqe_sample.png\" alt=\"plot\" width=\"900\">\n",
    "This is the same process as a large language model (LLM) building a sentence.  If the input sentence is \"The apple is colored...\" A logit would be produced that would sample the token \"red\" with high probability.  The GQE model similarly learns which unitaries help construct a circuit close to the ground state. \n",
    "\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 4:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "In the cell below, assume that the provided logit (w) was output from the GPT model.  Finish the function to compute the probability that each index $i=0$ to $i=9$ is sampled given an activation function of $e^{-\\beta w_i}$ is used (like in the paper).  $\\beta = \\frac{1}{T}$ is called the inverse temperature as it is related to a Boltzmann distribution from statistical mechanics in this content.  Start with $\\beta =1$.  What happens to the distribution when you change $\\beta$?  Why might it be helpful to adjust beta during GQE sampling?  \n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc13553-af1f-4d36-a199-18ef22c3ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_boltzmann_distribution(w, beta):\n",
    "    \"\"\"\n",
    "    Plots the energy vector w and the resulting probability distribution\n",
    "    for a given inverse temperature beta.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO\n",
    "    logits = -beta * np.array(w)\n",
    "    logits_shifted = logits - np.max(logits)\n",
    "    numerators = np.exp(logits_shifted)\n",
    "    probabilities = numerators / np.sum(numerators)\n",
    "#--------------Setup Plotting-------------------------\n",
    "    indices = np.arange(len(w))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    bars1 = ax1.bar(indices, w, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title(f'Logit Values', fontsize=14)\n",
    "    ax1.set_xlabel('Index ($j$)', fontsize=12)\n",
    "    ax1.set_ylabel('Value ($w_j$)', fontsize=12)\n",
    "    ax1.set_xticks(indices)\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                 f'{height:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "    colors = ['coral' if p == max(probabilities) else 'lightgreen' for p in probabilities]\n",
    "    bars2 = ax2.bar(indices, probabilities, color=colors, edgecolor='black')\n",
    "    \n",
    "    ax2.set_title(f'Probability Distribution ($\\\\beta={beta}$)', fontsize=14)\n",
    "    ax2.set_xlabel('Index ($j$)', fontsize=12)\n",
    "    ax2.set_ylabel('Probability $P(j) \\\\propto e^{-\\\\beta w_j}$', fontsize=12)\n",
    "    ax2.set_xticks(indices)\n",
    "    ax2.set_ylim(0, max(probabilities) * 1.15) # Add headroom for text\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        if height > 0.001: # Only label if visible\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                     f'{height:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.suptitle(f'Sampling with $\\\\beta={beta}$ (Inverse Temperature)', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "w = [1.0, 2.5, 0.5, 3.0, 1.5, 4.0, 0.2, 2.0, 5.0, 1.2]\n",
    "\n",
    "\n",
    "plot_boltzmann_distribution(w,0.1)\n",
    "plot_boltzmann_distribution(w,1)\n",
    "plot_boltzmann_distribution(w,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2757c7-46bb-40e1-bf37-62fc4ae89d8e",
   "metadata": {},
   "source": [
    "Now you have an understanding of how circuits are constructed with the GQE model, and we can revisit the training procedure.  For each training Epoch, GQE will sample a set of $C$ circuits. A quantum computer is then required to sample each circuit to compute an expectation value.  Each of these evaluations can be done asynchronously on many QPUs in parallel, a workflow that CUDA-Q enables. \n",
    "\n",
    "There are a number of ways to evaluate the cost function from this collection of energies including procedures called logit matching loss and group relative policy optimization-based loss, with the latter generally a better choice to avoid instances where the loss decreases but the energy plateaus. Curious readers can explore the details of these loss functions in the appendix of \"The Generative Quantum Eigensolver (GQE) and its Application for Ground State Search\".  \n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 5:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "This exercise will help you explore why GQE is more efficient and scalable compared to VQE. Consider the three plots below that demonstrate simulated comparison of GQE and VQE.  Comment on the accuracy of VQE vs GQE.  Now, complete the tables below to compare the quantum resources used between the two methods. Any user inputs are provided for you, and remember you can use functions from solvers to compute the number of UCCSD parameters.\n",
    "</div>\n",
    "\n",
    "| Molecule | BeH$_2$ | N$_2$ | CO$_2$ |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| **Active Space (e, o)** | (4,6) | (6,6) | (9,10) |\n",
    "| **VQE Parameters (UCCSD)** |  92| 117| 560 |\n",
    "| **VQE Epochs** |2000  | 3000| 10000 |\n",
    "| **VQE Circuit Evals. per Epoch** | 184 |234 | 1120 |\n",
    " **Total VQE Circuit Evals.** | 368000 | 702000|11200000  |\n",
    "| **GQE Epochs** | 500 |3000 | 5000 |\n",
    "| **GQE Evals. per Epochs** | 50 |50 | 50 |\n",
    "| **Total GQE Circuit Evals.** | 25000 | 150000| 250000 |\n",
    "| **GQE Total Circuit Evals. (% of VQE)** | 7 % | 21 %| 2 % |\n",
    "\n",
    "\n",
    "| Molecule | BeH$_2$ | N$_2$ | CO$_2$ |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| **Active Space (e, o)** | (4,6) | (6,6) | (9,10) |\n",
    "| **VQE Operators (Parameters) per Circuit** | 92 | 117 |560  |\n",
    "| **GQE Operators per Circuit** | 60 | 100| 200 |\n",
    "| **GQE Operators (% of VQE)** | 65 % | 85 %| 36 % |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d631fd-b4d9-4eb1-ba31-6482b6c3c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solvers.stateprep.get_num_uccsd_parameters(4,12))\n",
    "print(solvers.stateprep.get_num_uccsd_parameters(6,12))\n",
    "print(solvers.stateprep.get_num_uccsd_parameters(10,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983aefe-5270-4f11-998e-a60070cf1eb8",
   "metadata": {},
   "source": [
    "## Running GQE with CUDA-Q Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca48994-d8b4-4ba3-8836-41e1431d530a",
   "metadata": {},
   "source": [
    "\n",
    "GQE can be used in CUDA-Q Solvers but requires an additional install `pip install cudaq-solvers[gqe] -q`.\n",
    "\n",
    "Running the GQE method in CUDA-Q Solvers is simple and can be broken down into a few simple steps. First a molecular Hamiltonian is constructed just like you would do for VQE. Next, an operator pool is constructed. The built-in function `get_gqe_pauli_pool` takes a list of potential parameters and combines them with the base UCCSD operators to form a collection of different operators and evolution time steps.  Note, you can customize this if you want, but we will just use the built-in function. \n",
    "\n",
    "Next, a cost function is defined which utilizes a CUDA-Q kernel to apply the selected operators.\n",
    "\n",
    "Finally, configuration settings for the model are specified that determine its training behavior.  More on these later. \n",
    "\n",
    "The model then runs using the following API call: `solvers.gqe(cost, op_pool, config=cfg)` which produces the lowest energy circuit found by the routine and its corresponding operators.\n",
    "\n",
    "An entire code example can be found [here](https://nvidia.github.io/cudaqx/examples_rst/solvers/gqe.html) with all of these steps. Explore the script if you want to develop your own workflow using GQE.\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px; box-shadow: 0px 2px 4px rgba(0,0,0,0.1);\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0; margin-bottom: 10px;\">Exercise 6:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "To understand the GQE results, you will run a script called `run_gqe_h1.py` which is based off of the main workflow above, but with a few additional flags you can run to explore different settings in this notebook and easily plot the results.\n",
    "\n",
    "Each example below considers $H_2$ with a 6-31G basis set and a (2,3) active space. This is a simple molecule to make model training fast but also see some interesting trends.\n",
    "\n",
    "Important: you will change parameters and observe different model behavior.  We have set a random seed so results are reproducible. The trends we comment on will vary from system to system so the point is not to draw universal conclusions but see some of the challenges and considerations that arise with the GQE method.\n",
    "\n",
    "\n",
    "üíª Just a heads-up: The rest of this notebook is designed to be run on an environment with a GPU. If you don't have access to a GPU, feel free to run the cells under script calls to load the precomputed results. Enjoy learning! ‚≠ê\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616acef-d4db-42f5-8607-000193a3f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 5 --lr 1e-7 --temperature 5.0 --output_file 'baseline.png'\n",
    "display(Image(filename='baseline.png', width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3e86c-a861-436d-8816-3862dea5b70c",
   "metadata": {},
   "source": [
    "A few things to notice.  First, the final energy produced by the routine may not correspond to any of the circuits sampled in the final epoch.  This is because every epoch, 5 circuits are sampled and their energies measured. The final result is the lowest energy found from all  50 * 5 circuits sampled, even if it was found in an earlier epoch! In the instance above, that is epoch 36.  \n",
    "\n",
    "In theory, as the model trains, it samples better circuits, but the process is training a distribution so it is not garunteed the best solution will be sampled at any particular epoch. This can somewhat be observed as the samples get generally lower in energy.  The training loss function below shows that the model is indeed training and would probably get even better if we let it run longer. \n",
    "\n",
    "Also note that the data is output in a folder called `gqe_logs` which creates a JSON file with all of the data. Each epoch produces sets of index values that looks like this. \n",
    "\n",
    "$$[324, 205, 0, 324, 205, 135, 258, 221, 190, 205, 233, 146, 236, 324, 376, 31, 205, 135, 324, 461]$$ \n",
    "\n",
    "This specifies a circuit which is constructed by adding operator 324 then 205, and so on based on the operators in the vocabulary. Each circuit has an associated energy saved too.\n",
    "\n",
    "A final observation, note how many parameters are trained, over 86 million!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b4304-8288-4653-91a9-32dd6b1af320",
   "metadata": {},
   "source": [
    "### Exploring Sample Size\n",
    "\n",
    "Sample size is another key choice. Consider how sample size will impact the training and results. What is the downside to a higher sample count?  Try running with only 2 samples and then with 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3b323-e857-47c3-b335-c8fd06b142d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 2 --lr 1e-7 --temperature 5.0 --output_file 'sample_small.png'\n",
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 100 --lr 1e-7 --temperature 5.0 --output_file 'sample_large.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b188abb-0a55-4bed-b8bb-b05fde155a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='sample_small.png', width=700))\n",
    "display(Image(filename='baseline.png', width=700))\n",
    "display(Image(filename='sample_large.png', width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08712cbc-4e74-4157-a57a-54c4ad03fdfa",
   "metadata": {},
   "source": [
    "The key observation here is the convergence of the loss function. Notice that with high samples, it convergences more smoothly compared to only two samples and results in a better overall result. This makes intuitive sense. If the goal is to train a distribution, more samples will always provide a better representation of what is being trained.  The tradeoff is that more samples require more QPU time. So users must account for the resources they have available for training. Look at the timings shown for the training steps above. Notice how much longer it took to obtain 30 samples.  If this was a much larger circuit with more operators this would take even longer.\n",
    "\n",
    "Thankfully, the GQE method can be easily parallelized and run every sample asynchronously at the same time on a different QPU or GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f90f3-08f2-4c46-82e6-35f8ac61685d",
   "metadata": {},
   "source": [
    "### Exploring Gate Count\n",
    "\n",
    "You can also set the gate count to specify how many operators are sampled for each circuit. Run a couple of cases below with a small and large number of operators/gates.  What problems might occur with too many or too few?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6a89b-55ce-4fb5-94d3-b1953e5ffdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 4 --num_samples 5 --lr 1e-7 --temperature 5.0 --output_file 'gates_small.png'\n",
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 100 --num_samples 5 --lr 1e-7 --temperature 5.0 --output_file 'gates_large.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c2a25-bf5f-4ca7-bdbc-08c907c92b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='gates_small.png', width=600))\n",
    "display(Image(filename='baseline.png', width=600))\n",
    "display(Image(filename='gates_large.png', width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d8bab-48d6-41f5-b1c9-2afd0083dfe5",
   "metadata": {},
   "source": [
    "You might have thought that more gates would improve the results, but in this case, it is the opposite. It is true that more gates provide more flexibility, but it also becomes harder to learn.  Notice how the loss function is much less converged with 100 gates. This may improve with more iterations, but it also might mean that there is some overfitting going on and we do not require as many gates to construct a good wavefunction.\n",
    "\n",
    "It might also be the case that our vocabulary is the limiting factor and changing the operator pool is the fix.\n",
    "\n",
    "Note that for more complex systems, too few gates can easily be a problem. If the circuits do not have enough flexibility to describe the entanglement structure of the wavefunction, poor results are guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79e9d2-df56-4bb7-a992-20ab532291e0",
   "metadata": {},
   "source": [
    "### Exploring Temperature\n",
    "\n",
    "We can explore what happens when we change some of the settings.  A few cautions though.  First, the model training and sampling process is stochastic so any change we make is not garunteed to repeat what we observe. Our goal is to make some general observations about the impacts of changing hyperparameters like temperature and number of samples.  For our purposes, a fixed seed is used so the results are reproducible.\n",
    "\n",
    "First try increasing the temperature to 30.  What do you expect to see? Also try running a case which a much lower temperature of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9bdbc-245b-47d5-a7b4-80509ccb8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 5 --lr 1e-7 --temperature 0.5 --output_file 'low_temp.png'\n",
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 5 --lr 1e-7 --temperature 30 --output_file 'high_temp.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c1758-4809-4c4a-a80e-3e49da07c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='low_temp.png', width=600))\n",
    "display(Image(filename='baseline.png', width=600))\n",
    "display(Image(filename='high_temp.png', width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c72064-381c-4a9d-945e-96849496a112",
   "metadata": {},
   "source": [
    "The impact of temperature is slightly harder to connect to the result, but it can be observed in the plots.   Recall that the temperature tunes the probability that the model samples less likely operators when building the circuit.  Notice the spread of the samples. Clearly, they spread increases as the temperature is raised.  This is important for larger systems where the search space is enormous and there may be many local minima that yield poor solutions.  In the case of $H_2$ the system is simple so there is not as much impact from temperature on the result.\n",
    "\n",
    "Underneath the hood, GQE uses an adaptive temperature schedule that balances this tradeoff to ensure the balance between quality results and exploration of the Hilbert space by slowly increasing the temperature in later epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe0de8-d37f-4757-b107-278688ea9fc8",
   "metadata": {},
   "source": [
    "### Exploring Learning Rate\n",
    "\n",
    "The learning rate is another key parameter that shows up in many AI applications and essentially controls how much the neural network parameters are adjusted each epoch.  Try running the default setting with a high and low learning rate.  What do you expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7506119-6c14-4791-9e58-a6061562e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 5 --lr 1e-9 --temperature 5 --output_file 'low_lr.png'\n",
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 5 --lr 1e-4 --temperature 5 --output_file 'high_lr.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1168c-076d-49a1-9871-af21a864a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='low_lr.png', width=600))\n",
    "display(Image(filename='baseline.png', width=600))\n",
    "display(Image(filename='high_lr.png', width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5e4c7-4a7e-4e10-8adc-1c8537ea7ec2",
   "metadata": {},
   "source": [
    "Notice in the first plot, the learning rate is so small, the model does not really learn, and the cost goes up even! Essentially, the samples do not guide the training or will do so at such a slow rate it is impractical.  On the flip side, notice how a high learning rate sends massive shocks to the parameters resulting in some large spikes in the loss when higher energy circuits are sampled.  This can cause problems and even cause the model to overshoot the correct answer.  This is particularly a problem when the temperature is high, and the learning rate is high. Try running that case below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ed8ee-0e7c-4f58-bb9a-8a77623f4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../aux_files/vqe_and_gqe/run_gqe.h2.py --max_iters 50 --ngates 20 --num_samples 5 --lr 1e-3 --temperature 30 --output_file 'high_lr_temp.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4a4ce-e657-4b68-b034-693c2d10b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='high_lr_temp.png', width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad5b14-6d84-4d24-91a6-863c1f3d7560",
   "metadata": {},
   "source": [
    "In this case the training is a mess as the model is far too sensitive to higher energy samples.\n",
    "\n",
    "Learning rate is another careful balance of time and training quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cfe12e-07ce-4ca4-ad91-f68806f78b85",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The ground state problem is fundamental for chemistry and more broadly quantum computing.  In this notebook you learned how a prototypical variational method like VQE can help solve the problem but comes with a number of its own issues.  Much research is dedicated to solving this problem with clever techniques like AI.\n",
    "\n",
    "The GQE method is an excellent and pioneering example of an improved approach that can converge with fewer quantum circuit executions, can run in parallel, and result in a pretrained model that can be transferred to other contexts.   Current work is exploring how GQE can be trained on chemical system A and then sample circuits for system B with little or no additional training.  \n",
    "\n",
    "Likewise, GQE has already been applied to other domains like combinatorial optimization and shows great promise as a flexible and scalable quantum algorithm.  \n",
    "\n",
    "To explore more features of GQE, check out the CUDA-Q Solvers [documentation](https://nvidia.github.io/cudaqx/examples_rst/solvers/gqe.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
